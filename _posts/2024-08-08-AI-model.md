---
layout: article
title: "AI模型相关知识"
data: 2024-08-08
tags:
  - AI
  - 云计算

---


## 摘要

本文主要介绍AI模型相关知识，包括AI模型相关技术、AI模型相关工具、AI模型相关应用等。

## 一些概念

模型的泛化能力：模型举一反三的能力，即模型在测试集数据时的表现，而不是训练时数据的表现
大模型的幻觉问题：上下文矛盾问题、与prompt要求不一致、与事实矛盾（编造事实）、与常识矛盾（常识）
产生幻觉的原因：数据质量问题（不准确的信息、缺乏多样性），大模型训练的过拟合（泛化能力弱），推理过程，提升工程（给模型的prompt不够好）

## 大模型分类

📝 自然语言处理模型（NLP）：文本分类、命名实体识别、问答、语言建模、摘要、翻译、多项选择和文本生成。Transformer、BERT  
机器视觉（computer vision）：图像分类、目标检测和语义分割。CNN、ResNet  
🗣️ 音频（audio）：自动语音识别和音频分类。WaveNet、Tacotron  
📺 多模态模型（Multimodal）：表格问答、光学字符识别、从扫描文档提取信息、视频分类和视觉问答。CLIP、ViT  

## 大模型分布式并行

在单芯片或者加速卡上无法提供所需的算力和内存需求的情况下，考虑大模型分布式并行技术是一个重要的研究方向。
分布式并行分为数据并行、模型并行，模型并行又分为张量并行和流水线并行。

### 集合通信原语

在并行计算中，通信原语是指用于在不同计算节点或设备之间进行数据传输和同步的基本操作。一些常见的通信原语包括：

- All-reduce：将所有节点上的数据求和，然后广播到所有节点。
- All-gather：将所有节点上的数据收集到同一个节点。
- Broadcast：将一个节点上的数据广播到所有节点。
- Reduce：将所有节点上的数据进行某种操作（如求和、求平均、取最大值等）后，将结果发送回指定节点。这个操作常用于在并行计算中进行局部聚合。
- Scatter: 从一个节点的数据集合中将数据分发到其他节点上。通常用于将一个较大的数据集合分割成多个部分，然后分发到不同节点上进行并行处理。
- Gather: 将各个节点上的数据收集到一个节点上。通常用于将多个节点上的局部数据收集到一个节点上进行汇总或分析。

### 数据并行技术

1. Data parallelism, DP 数据并行
   数据并行是最简单的一种分布式并行技术，具体实施是将大规模数据集分割成多个小批量，每个批量被发送到不同的计算设备（如 NPU）上并行处理。每个计算设备拥有完整的模型副本，并单独计算梯度，然后通过 all_reduce 通信机制在计算设备上更新模型参数，以保持模型的一致性。

2. Distribution Data Parallel, DDP 分布式数据并行
   DDP 是一种分布式训练方法，它允许模型在多个计算节点上进行并行训练，每个节点都有自己的本地模型副本和本地数据。DDP 通常用于大规模的数据并行任务，其中模型参数在所有节点之间同步，但每个节点独立处理不同的数据批次。

3. Fully Sharded Data Parallel, FSDP 全分片数据并行
   Fully Sharded Data Parallelism (FSDP) 技术是 DP 和 DDP 技术的结合版本，可以实现更高效的模型训练和更好的横向扩展性。这种技术的核心思想是将神经网络的权重参数以及梯度信息进行分片（shard），并将这些分片分配到不同的设备或者计算节点上进行并行处理。FSDP 分享所有的模型参数，梯度，和优化状态。所以在计算的相应节点需要进行参数、梯度和优化状态数据的同步通信操作。
  
### 模型并行技术

模型的并行技术可以总结为张量并行和流水并行。

1. 张量并行
   将模型的张量操作分解成多个子张量操作，并且在不同的设备上并行执行这些操作。这样做的好处是可以将大模型的计算负载分布到多个设备上，从而提高模型的计算效率和训练速度。在张量并行中，需要考虑如何划分模型的不同层，并且设计合适的通信机制来在不同设备之间交换数据和同步参数。通常会使用诸如 All-reduce 等通信原语来实现梯度的聚合和参数的同步。
2. 流水线并行
   将模型的不同层划分成多个阶段，并且每个阶段在不同的设备上并行执行。每个设备负责计算模型的一部分，并将计算结果传递给下一个设备，形成一个计算流水线。在流水并行中，需要设计合适的数据流和通信机制来在不同设备之间传递数据和同步计算结果。通常会使用缓冲区和流水线控制器来管理数据流，并确保计算的正确性和一致性。

## 微调模型训练占用显存

### 常用微调

以微调1B模型为例，模型参数为16bit=2byte，则模型大小为2GB微调模型占用显存如下：

1. Model weight: 2GB
2. Gradient(梯度): 2GB
3. optimizer state: 8GB，不同优化器占用显存不同，可以保守估计为模型大小的4倍（Adam，SGD，也有资料说占用6倍）
4. Activation: 可以忽略不计

有资料认为：设模型参数为a，则全量微调占用显存约为20xa，LoRA微调占用显存约为1.3xa，推理约为2xa（按照FP16计算，占用2byte空间）

### 高效微调（PEFT），如LoRA

如LoRA方法，只微调adaptor，此时Gradient(梯度)和Optimizer state只占用原有显存量的2.5%

## 模型训练时间

模型的算力需求为：8x模型的参数x训练的token数，也有资料说为：6x模型的参数x训练的token数
GPU提供的算力为：单GPU算力xGPU数量xGPU利用率，GPU利用率一般达不到50%
模型训练时间 = 模型的算力需求/GPU提供的算力

## 大模型量化

量化的核心思想：降低参数精度，精度越低，占用内存空间越小，计算速度越快。

## 大模型蒸馏

用小模型模仿大模型

## RAG和微调的区别

RAG是LLM+知识，生成问题的答案，没有模型训练的过程  
微调是LLM+知识，改造原有模型，并生成新的模型来回答问题

### 场景

1. 动态数据：使用RAG
2. 模型能力定制：使用微调
3. 幻觉：RAG优于微调
4. 可解释性：RAG优于微调
5. 成本：RAG优于微调
6. 依赖通用能力：RAG优于微调
7. 低延迟场景：微调优于RAG
8. 智能设备：微调优于RAG

## 模型推理

## 模型评估指标

1. General
   1. MMLU
   2. MMLU-Pro
   3. AGIEval English
2. Knowledge reasoning
   1. TriviaQA-Wiki
3. Reading comprehension
   1. SQuAD
   2. QuAC
   3. BoolQ
   4. DROP