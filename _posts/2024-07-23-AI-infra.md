---
layout: article
title: "AI-infra"
data: 2024-07-23
tags:
  - AI
  - 云计算

---


## 摘要

AI智算中心的相关知识点

## GPU服务器

- HGX与DGX的区别：HGX是服务器中的GPU模块例如HGX H100则是包含8块H100的模块，DGX basePod则是包括GPU、内存、网卡、硬盘和HGX模块的AI服务器

## GPU

## 网络

NVIDIA Mellanox SHARP

## 存储

此处为引用，没有准确说明计算原理：

```

以OpenAI为例，进行样本数据量的分析。GPT2是40GB、GPT3是45TB、GPT4的量级预计达到了数百TB甚至PB级，对存储的空间也带来巨大挑战。

在checkpoint和模型文件部分：以GPT4为例，Checkpoint 数据大小约为5-8TB，模型大小约为2.8TB，按照常规的每四个小时保存一次，90天的训练周期，保存15%的比例，总的容量约为400TB。

模型文件也会有多个版本，总的大小预计也将超过100TB，整个数据量还是很大的，并且对读写的要求也非常高。

```

## NCCL

## MLperf

## 大模型显存占用分析

### 模型参数量与模型大小关系

- l层Transformer结构可训练参数量为l(12h^2)+Vh
- 模型使用FP16/BF16方式保存和加载到显存，那么每个参数占用2byte
- 模型使用FP32方式保存和加载到显存，那么每个参数占用4byte
- 大模型混合精度训练时，使用BF16进行前向传递，FP32反向传递梯度信息；优化器更新模型参数时，使用FP32优化器状态、FP32的梯度来更新模型参数，模型参数占用的内存为FP16+BF16=2*FP32
- 训练总内存=模型内存+梯度内存+优化器内存+激活内存+其他内存
- 大模型训练过程中最少需要8倍于模型权重的内存；65B的模型权重参数为130GB，训练时至少需要1TB的内存

| 模型名称 | 隐藏层维度h | 层数l | 12h^2 | 实际参数量 | 模型大小（FP16） |
| -------- | ----------- | ----- | ----- | ---------- | ---------------- |
| LLAMA-6B | 4096        | 32      | 6442450944 | 6.7B      | 12GB       |
| LLAMA-65B| 8192        | 80      | 64424509440 | 65.2B     | 120GB     |

### 模型推理显存分析

- 推理显存=模型内存+KV Cache内存+其他内存

### 模型训练时间计算

- 模型参数量（模型大小）和训练总tokens数（训练的数据）决定LLMs模型需要的计算量
- 给定计算量，训练时间跟GPU的型号数量和利用率相关
理论时间E=K*T*P/n*X*利用率

以LLaMa-65B为例，假设训练总tokens为1.4TB，GPU型号为A100，峰值性能为312TFLOPS，GPU卡数量为2048张，利用率为60%，那么训练时间
$E=8*(1.4*10^{12})*(65*10^9)/2048*(312*10^{12})*0.6=1898871s=21 days$

- 按照训练时间为30天计算，可训练模型（8*训练token数*模型参数）= GPUs*单GPU算力*GPU利用率*时间(s)= $4*91.6T*0.5*(3600*24*30)=474854400*1TFLOPS$
- 训练时间s=3600*24*30=2592000
- 按照训练tokens为10G计算，模型参数为6B
- 参考：https://cloud.baidu.com/qianfandev/topic/267367

## 电力

## 散热
